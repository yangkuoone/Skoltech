\documentclass{ITaSconf}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[russian]{babel}
\usepackage{url}
\usepackage{xcolor}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}

\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}

\usepackage{supertabular}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\newcommand\note[1]{ \textcolor{red}{|| #1 ||} }
\newcommand{\argmin}{\mathop{\rm arg\,min}\limits}
\newcommand{\argmax}{\mathop{\rm arg\,max}\limits}
\def\PP{\mathrm{P}}
\def\RR{\mathbb{R}}
\def\HH{\mathrm{H}}
\def\II{\mathrm{I}}

\title{Some overlapping community detection models\\ {\color{red} DRAFT}}
\author{
	Konstantin Slavnov \\
	\begin{affiliation}
		Skoltech
	\end{affiliation}\\
	\email{k.slavnov@skoltech.ru}
	\and
	Maxim Panov \\
	\begin{affiliation}
		Skoltech
	\end{affiliation}\\
	\email{panov.maxim@gmail.com}
}

\begin{document}
	\maketitle
	
	\begin{abstract}
		Basic summary for new overlapping community detection models. 
		Our aim to understand its difference and create own model with logistic loss function.
		We consider next models and algorithms: MMSB \cite{airoldi2008MMSB}, DC-MMSB \cite{karrer2011DCMMSB}, GeoNMF \cite{mao201GeoNMF}, OCCAM \cite{zhang2014OCCAM}, SAAC \cite{kaufmann2015SAAC}.
		There are also ``colored edges’' \cite{ball2011efficient}, AGM (BigCLAM) \cite{yang2012community}, \cite{yang2013overlapping} models existed.
	\end{abstract}
	
	\section{\color{red}  TODO:}
	{\color{red}
		\begin{enumerate}
			\item Sample splitting technique CProj algorithm.
			\item Add SAAC description
		\end{enumerate}
	}
	\section{Intro}
	
	Community detection is the greatest problem ever \cite{Fortunato10}!
	
	\section{Models}
	
	\paragraph{MMSB (Mixed membership stochastic blockmodel) \cite{airoldi2008MMSB}, description from \cite{mao201GeoNMF}.}
	One of the basic models in this field. And big amount of other models have the mixed membership stochastic block model or its variations in background. 
	
	In this model each node $i$ has a discrete probability distribution $\theta_i$ = $(\theta_{i1}, \dots , \theta_{iK})$ over $K$ clusters, and connections between nodes $i$ and $j$ are generated by first drawing from $\theta_i$ and $\theta_j$	independently, and then creating a connection based on cluster-specific matrix $B \in [0, 1]^{K\times K}$. Specifically,
	\begin{align*}
	&c_i \sim \theta_i, \\
	&c_j \sim \theta_j, \\ 	
	&P(A_{ij} = 1 \,|\, c_{i} , \, c_{j}) = B_{c_{i} , c_{j}},
	\end{align*}
	where $A$ represents the adjacency matrix of the generated graph. $B$ should have higher values on its diagonal as compared to the off-diagonal, implying that nodes in the same cluster are more likely to form an edge. 
	
	In matrix form we can consider the probability matrix of the network $P$. 
	\begin{align}
	\label{eq:P}
	P = \rho \Theta B \Theta^T,
	\end{align}
	
	where $\Theta \in \mathbf{R}^{n\times K}$ is the node-community distribution matrix, whose entries $\Theta_{ij}$ give the probability that node $i$ is in community $j$. The row vector $\Theta_i \, (i ∈ [n]) $ is the community membership distribution of node $i$, and sums to 1. The parameter $B \in [0, 1]^{K\times K}$ is the community-community interaction matrix, where $B_{ij}$ is the probability of having a link between node in community $i$ and node in community $j$. The parameter $\rho$ controls the sparsity of the graph and also needs for theoretical results. 
	
	Additionally, for each node $i \in [n]$, we draw $\Theta_i \sim \mathrm{Dir}(\alpha)$, where $\alpha \in \mathrm{R}^{K}$.
	
	The adjacency matrix of the graph is generated by: 
	$$
	A = \mathrm{Bernoulli}(P) = \mathrm{Bernoulli}(\rho \Theta B \Theta^T),
	$$
	
	\paragraph{GeoNMF (Geometric Non-negative matrix factorization) \cite{mao201GeoNMF}.} There are MMSB model and symmetric non-negative matrix factorization approach in background.
	
	It is tries to infer $W = \sqrt{\rho \Theta B^{1/2}}$ that factorizes $P=W W^T$ from MMSB model (\ref{eq:P}).
	$B \ge 0$ --- diagonal for theoretical result. Sample splitting technique \cite{mcsherry2001spectral}  and k-means clustering are used for algorithm construction. This approach is very close to \textbf{OCCAM}'s one. Later we describe intuitive understanding of method.\\
	
	\textbf{Algorithm:}
	\begin{enumerate}
		\item Randomly split the set of nodes into two equal-sized parts $S$ and $\bar{S}$.
		\item Get the rank-K eigen-decompositions $A(S, S) = \hat V_1 \hat E_1 \hat V_1^T$ and $A(\bar{S}, \bar{S}) = \hat V_2 \hat E_2 \hat V_2^T$ 
		\item Calculate degree matrices $D_{12}$ and $D_{21}$ for the rows of $A(S, \bar S)$ and $A(\bar S, S)$ respectively.
		\item $\hat X = D_{21}^{-1/2} \hat V_1 \hat E_1^{1/2}$
		\item $ \mathcal{T} =  \left\{ i\,:\, \lVert \hat X(i,:)\rVert_2 \ge \max_j \lVert \hat X(j,:)\rVert_2 \left( 1 - c \sqrt{\frac{\log n}{(n \rho^2)}} \right) \right\}$
		\item Run K-means clustering on $\hat X(\mathcal{T} , :)$, where each row is a point, then pick up only one point from each cluster to construct $S_p$.
		\item $\hat X_p = \hat X (S_p, :)$
		\item Get $\hat B = \mathrm{diag}(\hat B_i), \; \hat B_i = \lVert e^T_i D_{21}^{1/2}(S_p, S_p) \hat X_p \rVert^2_F, \; i \in [K]$
		\item $\hat \rho = \max_i \hat B_i$
		\item $\hat B = \hat B / \hat \rho$
		\item $\Theta(\bar S) = D_{21}^{1/2} \hat X \hat X_p^{-1} D_{21}^{-1/2}(S_p, S_p)$
		\item Repeat steps with $D_{12}$, $\hat V_2$, and $\hat E_2$ to obtain $\hat \Theta(S)$.
		
	\end{enumerate}
	
	\paragraph{Sample splitting technique \cite{mcsherry2001spectral}.}
	
	This is technique can be applied to many graph problems. Here we describe the approach on graph partitioning task.
	
	Consider simple stochastic block model $\mathcal{G}(\psi, P)$ for graph $G$. Let $ \psi \; : \; \{1, \dots, n\} \rightarrow\{1,\dots,k\} $ be a partition of $n$ nodes to $k$ classes. Let $P$ be a $k\times k$ matrix, where $P_{ij} \in [0,1]$ for all $i,j$. Include edge $(u,v)$ with probability $P_{\psi(u)\psi(v)}$. Denote $G_{uv} = P_{\psi(u)\psi(v)}$ and $\hat G \sim \mathcal{G}(\psi, P)$ --- sampled adjacency matrix.
	
	Now we can formulate \textbf{Planted Partition Problem:} Given a graph $\hat G$, produce a partition $\hat\phi \; : \; \hat\phi(u) = \hat\phi(v) \quad \mathrm{iff} \quad \phi(u) = \phi(v)$.
	
	If we have $G$, it is easy to reconstruct $\psi$ by clustering columns of $G$. Also we know several facts:\\
	
	For any $\psi$ and $P$, $\mathrm{rank}(G) = k$. 
	
	If $P_G$ is the projection on the column space of $G$:
	\begin{align*}
	&\left|  P_G(G_u) - G_u \right| = 0 \\
	&\left|  P_G(G_u) - \hat G_u \right| = \varepsilon \text{ is small}.
	\end{align*}
	
	We do not have access to $P_G$. So, the approach is to approximate $P_G$ by $P_X$ projector so that:
	\begin{align*}
	&\left|P_X(G_u) - G_u \right| \text{ is small}\\
	&\left|P_X(G_u - \hat G_u) \right| \text{ is small}\\
	\end{align*}
	
	Main conclusion that $\left| P_X(\hat G_u) - G_u \right| = \varepsilon$ is small. If $|G_u - G_v|$ for $\phi(u) \ne \phi(v)$ is much larger then approximation error $\varepsilon$ we can use simple clustering method to the $P_X(\hat G_u)$.
	
	Let $\tau$ be clustering threshold parameter and $\mathbf{CProj}$ be a function for projection matrix computing. Also we will split $ \hat G$ matrix into two parts. This is done to avoid the conditioning between the error $| \hat G - G |$ and $\mathbf{CProj}(\hat G)$.\\
	
	\textbf{Algorithm:}
	\begin{enumerate}
		\item Randomly divide vertex set $\{1,\dots, n\}$ into two parts. Then columns of $\hat G$ will split as $\left[\,\hat A\, |\, \hat B\, \right]$.
		\item Build 2 projectors: $P_{\hat B} = \mathbf{CProj}(\hat B), \; P_{\hat A} = \mathbf{CProj}(\hat A)$.
		\item $\hat H = \left[\,P_{\hat B}(\hat A)\,|\,P_{\hat A}(\hat B)\,  \right]$. Here we see splitting effect: no $P_{\hat G}(\hat G)$ estimations.
		\item While there are unpartitioned nodes
		\begin{enumerate}
			\item Choose an unpartitioned node $u_i$ arbitrary.
			\item For each node $v$ set $\hat \psi (v) = i$ if $ | \hat H_{u_i} - \hat H_v | \le \tau$.
		\end{enumerate}
		\item Return partition $\hat \psi$.
		
	\end{enumerate}
	
	Also we can understand this splitting is technique to avoid overfitting ( or avoid build projector and run clusterisation on the same data).
	
	So now we should understand how $\mathbf{CProj}$ works. Notation $\hat A^T_v$ is the $v$th column of the transpose of $\hat A$, which is vector of roughly $n/2$ coordinates.\\
	
	\textbf{Algorithm} $\mathbf{CProj}(\hat A, k, s_m, \tau)$ {\color{red} not clear}:
	\begin{enumerate}
		\item While there are at least $s_m / 2$ unclassified nodes
		\begin{enumerate}
			\item Choose an unclassified node $v_i$ randomly.
			\item Let $T_i = \{ u \,: \, |P_{\hat A^T}(\hat A_{v_i}^T - \hat A_{u}^T)| \le  \tau\}$.
			\item Mark each $u \in T_i$ as classified.
		\end{enumerate}
		\item Assign each remaining node to the $T_i$ with closest projected $v_i$.
		\item Let $\hat c_i$ be the characteristic vector of $T_i$.
		\item Return $P_{\hat c}$ --- the projection onto the span of the $\hat c_i$
	\end{enumerate}
	
	If the $\hat c_i$ were characteristic vectors of $\psi$, this projection would be exactly $P_A$. Instead, we will see that the $\hat c_i$ are not unlike the characteristic vectors of $\psi$.
	
	For $\tau$ we know theoretical formula.
	
	\paragraph{DCSBM (Degree-corrected stochastic blockmodel) \cite{karrer2011DCMMSB}, description from \cite{zhang2014OCCAM}.}
	In MMSB (also as in just stochastic blockmodel), a node’s community determines its behavior entirely, and thus all nodes in the same community are “stochastically equivalent”, and in particular have the same expected degree. This is known to be often violated in practice, due to commonly present “hub” nodes with many more connections than other nodes	in their community. Or just free-scale distribution on node's degrees. The degree-corrected stochastic block model (DCSBM) (Karrer and Newman, 2011) was proposed to address this limitation, which multiplies the probability of an edge between	nodes $i$ and $j$ by the product of node-specific positive “degree parameters” $d_i$, $d_j$.
	
	In DCSBM we represent the probability matrix of the network in following way:
	\begin{align}
	\label{eq:P2}
	P = \rho D \Theta B \Theta^T D,
	\end{align}
	The $n \times n$ diagonal matrix $D = \mathrm{diag}(d_1, \dots, d_n)$ contains non-negative degree correction terms that allow for heterogeneity in the node degrees. There is main differences between two models.
	
	\paragraph{OCCAM (The overlapping continuous community assignment model). \cite{zhang2014OCCAM}, presentation \cite{Levina2014pres}}
	
	This model is similar to GeoNMF, but instead of MMSB model it uses OCCAM. Main approach remains the same.
	Suppose that $P$ factorizes $P=W W^T$. Thus, from (\ref{eq:P2}) we have $W = \sqrt \rho D \Theta B^{1/2}$. $B \succ 0, \; B_{kk} = 1$ in this model.
	
	Main intuition is follows. Let's ignore $\Theta$ for now. Consider a pure node from community $k$. Pure means that the node is contained in only one community: $\Theta_i = (0, \dots, 0, 1,0,\dots,0)$. Then $W_i = \Theta_i B^{1/2} = (B^{1/2})_k$ --- $k$-th column of $B^{1/2}$ matrix. Thus, $B^{1/2}$ is equal to latent positions of pure nodes. We can think about this vertex as community centers. Any row of $W$ is a linear combination of community centers with coefficients $\Theta_i$. So we have vector embedding for each vertex in pure node basis. 
	
	Here we have strategy for solving the problem.
	First, find community centers, i.e. estimate the rows of $B^{1/2}$. Second, project rows of W onto span of $B^{1/2}$ rows to estimate $\Theta$.\\
	
	\textbf{Algorithm:}
	\begin{enumerate}
		\item Compute the leading $K$ eigenvectors $U$ and eigenvalues $\Lambda$ of $A$, set $\hat W = U\Lambda^{1/2}$ (so $A \approx \hat{W}\hat{W}^T$)
		\item Normalize rows of $\hat W$. $\hat W^*_i = \dfrac{\hat W_i}{\|\hat W_i\| + \tau_n}$.
		\item Apply $K$-medians clustering to rows of $\hat W^*$ to estimate community centers, i.e. positions of pure nodes: $\hat S = \{\hat s_1,\dots,\hat s_K \}$.
		\item Project the rows of $\hat W^*$ onto $\hat S$ to obtain the coefficients $$ \tilde \Theta = \hat W^* \hat S ^T (S S^T)^{-1}. $$ and normalize rows to obtain the final membership vectors $\hat \Theta_i = \dfrac{\tilde \Theta_i}{\|\tilde\Theta_i\|_2}$.
	\end{enumerate}
	
	Here $\tau_n > 0$ is a small regularizer ensuring numerical stability in practice and concentration in theory.
	
	\paragraph{SMBO (Stochastic block model with overlaps) description from  \cite{kaufmann2015SAAC}.}
	One another approach to generalize Stochastic block model to overlapping community case. It is very close to MMSB model, but has had community affiliation: 
	$$
	\theta \in \{0,1\}^{n\times K}.
	$$
	And 
	$$
	p = \rho \Theta B \Theta^T, B \in [0;\,1]^{K \times K}.
	$$
	It is not difficult to understand that SMBO with $K$ communities is equal to SBM \cite{holland1983SBM} with up to $2^K$ communities.
	Next method use this generalization to recover community structure.
	
	\paragraph{SAAC (Spectral algorithm with additive clustering) \cite{kaufmann2015SAAC}.}
	SMBO is in background (see previous section).
	
	It has links to AGM, OCCAM and other models. {\color{red} add about it.} Uses USVT for eigenvectors selection \cite{chatterjee2015matrix}. \\
	
	\textbf{Algorithm:}
	\begin{enumerate}
		\item Selection of the eigenvectors. Form $\hat U$ a matrix whose columns are $\hat K$ eigenvectors of $A$ associated to eigenvalues $\lambda$ satisfying condition from USVT:
		$$
		|\lambda| > \sqrt{2(1+\nu) \hat d_{\max} \log(4n^{1+r})}.
		$$
		\item Initialization. $\hat \Theta = 0 \in \mathbf{R}^{n \times \hat K}$. $\hat B \in \mathbb{R}^{\hat K \times \hat K}$ initialized with k-means++ applied to $\hat U$, the first centroids being chosen at random	among nodes with degree smaller than the median degree.
		\item \textbf{while} ( $Loss - \|\hat U - \hat \Theta \hat B\|^2_F > \varepsilon$) \textbf{do}
		\item \qquad $Loss = \|\hat U - \hat \Theta \hat B\|^2_F$
		\item \qquad Update membership vectors (like k-means): $$\forall i:\quad \hat \Theta_i = \argmin_{\theta \in [0,1]^{\hat K}:\,1\le\|z\|_1\le m} \| \hat{U}_i - \theta \hat X\|.$$
		\item \qquad Update centroids: $\hat B = (\hat \Theta^T \hat \Theta)^{-1} \hat \Theta ^T \hat U$.
	\end{enumerate}
	Here k-means++ is initialization. It is a randomized procedure that picks as initial
	centroids rows from $\hat U$ that should be far from each other \cite{arthur2007k}.
	
	
	\bibliographystyle{amsplain}
	\bibliography{bibl}
	
\end{document}