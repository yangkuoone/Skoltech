{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perceptron is a linear model\n",
    "* Why?\n",
    "* Inherently simple - two solutions. \n",
    "  - Transform the features to make the problem linearly separable\n",
    "  - Make the network more complex\n",
    "* The **multi-layer perceptron** builds on the second idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning with the perceptron occurs in the weights\n",
    "* To make a more complex network, add more weights\n",
    "* Two ways to do so\n",
    "  - Add some backward connections - so neurons connect to inputs again\n",
    "  - Add more neurons\n",
    "* The first approach leads to **recurrent networks**\n",
    "* The second approach leads to MLP\n",
    "* It will allow us to add more \"layers\" of nuerons between the input nodes and the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show that we can solve the XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Worked in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Going *forward* means working out the outputs for given inputs\n",
    "* This is the *recall* phase we discussed last time\n",
    "* This is the same in the MLP as the Perceptron\n",
    "* Only we do it twice now - layer-by-layer\n",
    "* The activations of one layer of nodes are the inputs to the next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going Backwards: Back-Propagation of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computing the errors is the same\n",
    "* What to do with them - how to update the weights - is more difficult\n",
    "* This is called **back-propagation of error**\n",
    "* We do so through **gradient descent**\n",
    "* We know we need to update the weights, but which ones? Inputs $\\rightarrow$ hidden layer or hidden layer $\\rightarrow$ outputs?\n",
    "* Recall that our error function in the Perceptron was $E=t-y$\n",
    "* For the MLP we will use the familiar sum of squares error $E(t,y)=\\frac{1}{2}\\sum_{k=1}^n(t_k-y_k)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We also need to revisit our activation function\n",
    "* For the Perceptron, we used a binary activation function\n",
    "* This is not differentiable\n",
    "* For the MLP, we will use a *sigmoid* function\n",
    "* You have seen an exmaple in the logistic function with the now-familiar S-shape\n",
    "$$g(h)=\\frac{1}{1+e^{-\\beta h}}$$\n",
    "\n",
    "where $\\beta > 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "g_func = lambda h, beta : 1/(1 + np.exp(-beta*h))\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "ax.plot(x, g_func(x, 2))\n",
    "ax.hlines([0, 1], -3, 3, color=\"r\")\n",
    "ax.set_ylim(-.1, 1.1);\n",
    "ax.set_title(r\"$g(h;\\, \\beta)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We feed our inputs forward through the network, which tells us which nodes are firing\n",
    "* We compute the errors\n",
    "* We need to compute the gradient of the errors with respect to the weights\n",
    "* This allows us to update the weights in the \"downhill\" direction\n",
    "  * Do this for the nodes connected to the output layer\n",
    "  * Work backwards until we get back to the inputs again\n",
    "* Two problems\n",
    "  * We don't know the inputs to the output neurons\n",
    "  * We don't know the targets for the hidden neurons (for multiple hidden layers - don't know the inputs or the outputs...)\n",
    "* We can use the chain-rule from calculus to get around this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Algorithm Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* An input vector is put into the input nodes\n",
    "* Inputs are fed *forward* through the network\n",
    "  * Inputs and first-layer weights $v$ determine wither the hidden nodes fire via $g(\\cdot)$\n",
    "  * Outputs of these nodes and the second-layer weights are used to decide if the output neurons fire\n",
    "* Sum-of-squares error is computed\n",
    "* Error is fed backwards through the network\n",
    "  * Second-layer weights are updated (using $\\delta_0$)\n",
    "  * First-layer weights are updated (using $\\delta_h$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialization of weights\n",
    "* **Training** (Repeat)\n",
    "  * For each input vector\n",
    "    \n",
    "    **Forwards phase**\n",
    "    \n",
    "     * Compute the activation of each neuron $j$ in the hidden layer(s) using\n",
    "     <br /><br />\n",
    "    \n",
    "    $$h_j=\\sum_ix_iv_{ij}$$\n",
    "    $$a_j=g(h_j)=\\frac{1}{1+e^{-\\beta h_j}}$$\n",
    "    \n",
    "    <br />\n",
    "    \n",
    "     * Work through network until you get the output layers\n",
    "        \n",
    "    <br />\n",
    "    \n",
    "    $$h_k=\\sum_j{a_jw_{jk}}$$\n",
    "    $$y_k=g(h_k)=\\frac{1}{1+e^{-\\beta h_k}}$$\n",
    "    \n",
    "    **Backwards phase**\n",
    "    \n",
    "     * compute the error at the output using\n",
    "        \n",
    "        $$\\delta_{ok}=(t_k-y_k)y_k(1-y_k)$$\n",
    "     \n",
    "     * compute the error in the hidden layer(s) using\n",
    "        \n",
    "        $$\\delta_{hj}=a_j(1-a_j)\\sum_k w_{jk}\\delta_{ok}$$\n",
    "      \n",
    "     * update the output layer weights using\n",
    "        \n",
    "        $$w_{jk}\\leftarrow w_{jk} + \\eta \\delta_{ok}a_j$$\n",
    "        \n",
    "     * update the hidden layer weights using\n",
    "        \n",
    "        $$v_{ij}\\leftarrow v_{ij}+\\eta \\delta_{hj}x_i$$\n",
    "<br/>\n",
    "\n",
    "    Randomize the order of the input vectors so that you don't train in exactly the same order each iteration\n",
    "    \n",
    "    \n",
    "* **Recall**\n",
    "  * Use the Forwards Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch vs. Online algorithms**\n",
    "\n",
    "* The implementation below is what's called a **batch** algorithm\n",
    "* This means that the weights are only update after all the training inputs have been seen\n",
    "* The weights are updated once for each **epoch** (pass through training examples)\n",
    "* The gradient estimation will be more accurate and will thus converge to the local minimum more quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The algorithm described above is an **online** algorithm\n",
    "* It is **sequential**\n",
    "* An online algorithm would update the weights incrementally over the training inputs\n",
    "* Online algorithms have a few advantages \n",
    "  * They can be more efficient in terms of memory use\n",
    "  * They can stop \"early\"\n",
    "  * They can avoid local minimum by using less accurate gradients\n",
    "* Online algorithms are not always (easily) available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial weights**\n",
    "\n",
    "* Each neuron gets an input from $n$ different places (inputs nodes or hidden neurons)\n",
    "* If they all have the same variance, then the typical input for each neuron is $w\\sqrt{n}$\n",
    "* A common method is then to set the weights so that $\\frac{-1}{n}<w<\\frac{1}{n}$\n",
    "* This is called **uniform learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using different activation functions**\n",
    "\n",
    "* Regression problems: linear activation\n",
    "\n",
    "$$y_k = g(h_k)=h_k$$\n",
    "\n",
    "* This needs a new delta term for the update step\n",
    "\n",
    "$$\\delta_{ok}=(t_k-y_k)$$\n",
    "    \n",
    "* $1\\mbox{-of-}N$ output encoding: soft-max\n",
    "* $1\\mbox{-of-}N$ output encoding is used when the output variable can take on more than two values\n",
    "* For example, modeling the decision of transportation, \"bus\", \"car\", or \"bike\" might be encoded\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "    [[1, 0, 0],\n",
    "     [0, 1, 0],\n",
    "     [0, 0, 1]]\n",
    "\n",
    "* The soft-max function is the same function that appears in Multinomial Logit problems in statistics\n",
    "\n",
    "$$y_k = g(h_k) = \\frac{e^{h_k}}{\\sum_{h_k}e^{h_k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Minima**\n",
    "\n",
    "* As we saw when discussing optimization, local minima can be a problem \n",
    "* This is also the case for gradient descent\n",
    "* The problem is exacerbated by the higher dimensionality\n",
    "* One way to try to overcome getting stuck in local minima is by picking up **momentum**\n",
    "* Momentum also allows us to use a smaller (and thus more stable) learning rate $\\eta$\n",
    "* We add momentum to the weights updating as follows\n",
    "\n",
    "$$w_{ij}^t\\leftarrow w_{ij}^{t-1}+\\eta \\delta_0 a_j + \\alpha \\Delta w_{ij}^{t-1}$$\n",
    "\n",
    "where $0 < \\alpha < 1$ is the momentum constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from perceptron import Perceptron, add_bias_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for internal use to avoid [spaghetti code](http://en.wikipedia.org/wiki/Spaghetti_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _linear_delta(targets, outputs, nobs):\n",
    "    return (targets - outputs)/nobs\n",
    "\n",
    "def _logistic_delta(targets, outputs, *args):\n",
    "    return (targets - outputs)*outputs\n",
    "\n",
    "def _softmax_delta(targets, outputs, nobs):\n",
    "    return (targets - outputs)/nobs\n",
    "\n",
    "_calc_deltao = {\n",
    "        \"linear\" : _linear_delta,\n",
    "        \"logistic\" : _logistic_delta,\n",
    "        \"softmax\" : _softmax_delta\n",
    "        }\n",
    "\n",
    "def _linear_activation(outputs, *args):\n",
    "    return outputs\n",
    "\n",
    "def _logistic_activation(outputs, beta, *args):\n",
    "    return 1/(1+np.exp(-beta*outputs))\n",
    "\n",
    "def _softmax_activation(outputs, *args):\n",
    "    # this is multinomial logit\n",
    "    eX = np.exp(outputs)\n",
    "    return eX/eX.sum(axis=1)[:,None]\n",
    "\n",
    "\n",
    "_activation_funcs = {\n",
    "        \"linear\" : _linear_activation,\n",
    "        \"logistic\" : _logistic_activation,\n",
    "        \"softmax\" : _softmax_activation,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from perceptron import Perceptron, add_bias_node\n",
    "\n",
    "class MLP(Perceptron):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron\n",
    "    \"\"\"\n",
    "    def __init__(self, nhidden, eta, beta=1, momentum=0.9, outtype='logistic'):\n",
    "        # Set up network size\n",
    "        self.nhidden = nhidden\n",
    "        self.eta = eta\n",
    "\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.outtype = outtype\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialise network\n",
    "        weights1 = np.random.rand(self.m+1, self.nhidden)-0.5\n",
    "        weights1 *= 2/np.sqrt(self.m)\n",
    "        weights2 = np.random.rand(self.nhidden+1,self.n)-0.5\n",
    "        weights2 *= 2/np.sqrt(self.nhidden)\n",
    "\n",
    "        self.weights1 = weights1\n",
    "        self.weights2 = weights2\n",
    "\n",
    "    def earlystopping(self, inputs, targets, valid_input, valid_target,\n",
    "                            max_iter=100, epsilon=1e-3, disp=True):\n",
    "\n",
    "        self._initialize(inputs, targets)\n",
    "        valid_input = add_bias_node(valid_input)\n",
    "\n",
    "        # 2 iterations ago, last iteration, current iteration\n",
    "        # current iteration, last iteration, 2 iterations ago\n",
    "        last_errors = [0, np.inf, np.inf]\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        while np.any(np.diff(last_errors) > epsilon):\n",
    "            count += 1\n",
    "            if disp:\n",
    "                print count\n",
    "\n",
    "            # train the network\n",
    "            self.fit(inputs, targets, max_iter, init=False, disp=disp)\n",
    "            last_errors[2] = last_errors[1]\n",
    "            last_errors[1] = last_errors[0]\n",
    "\n",
    "            # check on the validation set\n",
    "            valid_output = self.predict(valid_input, add_bias=False)\n",
    "            errors = valid_target - valid_output\n",
    "            last_errors[0] = 0.5*np.sum(errors**2)\n",
    "        \n",
    "        if disp:\n",
    "            print \"Stopped in %d iterations\" % count, last_errors\n",
    "        return last_errors[0]\n",
    "\n",
    "    def fit(self, inputs, targets, max_iter, disp=True, init=True):\n",
    "        \"\"\"\n",
    "        Train the network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : array-like\n",
    "            The inputs data\n",
    "        targets : array-like\n",
    "            The targets to train on\n",
    "        max_iter : int\n",
    "            The number of iterations to perform\n",
    "        init : bool\n",
    "            Whether to initialize the weights or not.\n",
    "        \"\"\"\n",
    "        if init:\n",
    "            self._initialize(inputs, targets)\n",
    "        inputs = self.inputs\n",
    "        targets = self.targets\n",
    "        weights1 = self.weights1\n",
    "        weights2 = self.weights2\n",
    "        eta = self.eta\n",
    "        momentum = self.momentum\n",
    "        nobs = self.nobs\n",
    "\n",
    "        outtype = self.outtype\n",
    "\n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = add_bias_node(inputs)\n",
    "        change = range(self.nobs)\n",
    "\n",
    "        updatew1 = np.zeros_like(weights1)\n",
    "        updatew2 = np.zeros_like(weights2)\n",
    "\n",
    "\n",
    "        for n in range(1, max_iter+1):\n",
    "\n",
    "            # predict attaches hidden\n",
    "            outputs = self.predict(inputs, add_bias=False)\n",
    "\n",
    "            error = targets - outputs\n",
    "            obj = .5 * np.sum(error**2)\n",
    "\n",
    "            # Different types of output neurons\n",
    "            deltao = _calc_deltao[outtype](targets, outputs, nobs)\n",
    "            hidden = self.hidden\n",
    "            deltah = hidden * (1. - hidden) * np.dot(deltao, weights2.T)\n",
    "\n",
    "            updatew1 = (eta*(np.dot(inputs.T, deltah[:,:-1])) +\n",
    "                        momentum*updatew1)\n",
    "            updatew2 = (eta*(np.dot(self.hidden.T, deltao)) +\n",
    "                        momentum*updatew2)\n",
    "            weights1 += updatew1\n",
    "            weights2 += updatew2\n",
    "\n",
    "            # Randomise order of inputs\n",
    "            np.random.shuffle(change)\n",
    "            inputs = inputs[change,:]\n",
    "            targets = targets[change,:]\n",
    "\n",
    "        if disp:\n",
    "            print \"Iteration: \", n, \" Objective: \", obj\n",
    "\n",
    "        # attach results\n",
    "        self.weights1 = weights1\n",
    "        self.weights2 = weights2\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def predict(self, inputs=None, add_bias=True):\n",
    "        \"\"\"\n",
    "        Run the network forward.\n",
    "        \"\"\"\n",
    "        if inputs is None:\n",
    "            inputs = self.inputs\n",
    "\n",
    "        if add_bias:\n",
    "            inputs = add_bias_node(inputs)\n",
    "\n",
    "        hidden = np.dot(inputs, self.weights1)\n",
    "        hidden = _activation_funcs[\"logistic\"](hidden, self.beta)\n",
    "        hidden = add_bias_node(hidden)\n",
    "        self.hidden = hidden\n",
    "\n",
    "        outputs = np.dot(self.hidden, self.weights2)\n",
    "\n",
    "        outtype = self.outtype\n",
    "\n",
    "        # Different types of output neurons\n",
    "        return _activation_funcs[self.outtype](outputs, self.beta)\n",
    "\n",
    "    def confusion_matrix(self, inputs, targets, summary=True):\n",
    "        \"\"\"\n",
    "        Confusion matrix.\n",
    "        \"\"\"\n",
    "        targets = np.asarray(targets)\n",
    "\n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = add_bias_node(inputs)\n",
    "        # predict attaches hidden\n",
    "        outputs = self.predict(inputs, add_bias=False)\n",
    "\n",
    "        n_classes = targets.ndim == 1 and 1 or targets.shape[1]\n",
    "\n",
    "        if n_classes==1:\n",
    "            n_classes = 2\n",
    "            # 50% cut-off with continuous activation function\n",
    "            outputs = np.where(outputs > 0.5, 1, 0)\n",
    "        else:\n",
    "            # 1-of-N encoding\n",
    "            outputs = np.argmax(outputs, 1)\n",
    "            targets = np.argmax(targets, 1)\n",
    "\n",
    "        outputs = np.squeeze(outputs)\n",
    "        targets = np.squeeze(targets)\n",
    "\n",
    "        cm = np.histogram2d(targets, outputs, bins=n_classes)[0]\n",
    "\n",
    "        if not summary:\n",
    "            return cm\n",
    "        else:\n",
    "            return np.trace(cm)/np.sum(cm)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [[0., 0.],\n",
    "     [0., 1.],\n",
    "     [1., 0.],\n",
    "     [1., 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "and_clf = MLP(2, .25)\n",
    "and_clf.fit(X, target, 5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "and_clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "and_clf.confusion_matrix(X, target, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = [0., 1., 1., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xor_clf = MLP(2, .25)\n",
    "xor_clf.fit(X, target, 5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xor_clf.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Preparation\n",
    "* How much training data?\n",
    "* Number of hidden layers?\n",
    "* Overfitting\n",
    "   * Stop before overfitting\n",
    "   * How do we figure this out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, Testing, and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To evaluate our training, we may use the **holdout method**\n",
    "* We set aside some data from the training training set\n",
    "* This data is called the **test set**\n",
    "* It reduces the data available for training\n",
    "* We also want to try to evaluate how well the learning is going during training\n",
    "* For this we use a **validation set** (the same idea as **cross-validation** in statistics)\n",
    "* We might split the data into training:test:validation according to 50:25:25 or 60:20:20\n",
    "* This is called a **three-way data split**\n",
    "* You may need to be sure that the data is randomly ordered before splitting\n",
    "* All data preprocessing occurs before splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is common to have too little data for a proper training-testing-validation split\n",
    "* We might also want to avoid getting a small error rate because of \"bad\" random split\n",
    "* At the expense of more computations, we may try cross-validation\n",
    "  * Random Subsampling\n",
    "  * K-Fold Cross-Validation\n",
    "  * Leave-one-out Cross Validation\n",
    "* The idea is to randomly partition the dataset into $K$ subsets\n",
    "* Use one of the $K$ subsets as a validation set and train on the others\n",
    "* Do this again, leaving out another subset for validation, until all are left-out for validation\n",
    "* Use the model with the smallest validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So far we have trained a network for a fixed number of iterations\n",
    "* This is never what you really want to do\n",
    "* You run the potential for both over- and under-fitting the data.\n",
    "* We can use the **validation set** to determine when to stop\n",
    "* Run the network for some fixed number of iterations and then evaluate on the validation set\n",
    "* Run for a few more iterations, then start over\n",
    "* At some point the error on the validation set will start to increase\n",
    "* This is when we stop. Unsurprisingly, this is called **early stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First let's generate some data\n",
    "* Then we'll use the MLP to try to uncover the function (or data generating process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "x = np.linspace(0, 1, 40)[:,None] # make 2D\n",
    "t = (np.sin(2*np.pi*x) + \n",
    "     np.cos(4*np.pi*x) + \n",
    "     np.random.normal(0, .2, size=(40,1)))\n",
    "x = (x - .5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, t, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into 50:25:25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = x[::2]\n",
    "test = x[1::4]\n",
    "validate = x[3::4]\n",
    "\n",
    "train_target = t[::2]\n",
    "test_target = t[1::4]\n",
    "validate_target = t[3::4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We don't know how many hidden neurons, we'll need, so let's try 3.\n",
    "* We will run this for 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = MLP(3, .25, outtype=\"linear\")\n",
    "net.fit(train, train_target, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First let's decide how long to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.earlystopping(train, train_target, validate, validate_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we need to figure out how to select the number of nodes we want\n",
    "* To find out, we can run each network size a number of times, say 50 and keep the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nhiddens = [1, 2, 3, 5, 10, 25, 50, 100]\n",
    "all_errors = []\n",
    "nruns = 10\n",
    "\n",
    "for nhidden in nhiddens:\n",
    "    errors = []\n",
    "    mlp = MLP(nhidden, .25, outtype=\"linear\")\n",
    "    for i in range(nruns):\n",
    "        error = mlp.earlystopping(train, train_target, validate, validate_target, disp=False)\n",
    "        errors.append(error)\n",
    "    all_errors.append(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_errors = np.array(all_errors)\n",
    "print \"        n              mean          std            min            max\"\n",
    "print np.column_stack((nhiddens, all_errors.mean(1), all_errors.std(1),\n",
    "                       all_errors.min(1), all_errors.max(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn for training, testing, and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "a, b = np.arange(16).reshape((8, 2)), np.arange(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(a_train, \n",
    " a_test, \n",
    " b_train, \n",
    " b_test) = cross_validation.train_test_split(a, b, \n",
    "                                     train_size=.75,\n",
    "                                     random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_train, b_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_test, b_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside:** Python Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Folds Cross-Validation**\n",
    "\n",
    "* The data is split into $K$ consecutive \"folds\"\n",
    "* Of the K subsamples, retain a single subsample as a validation set\n",
    "* Use the other k-1 subsamples as a training set\n",
    "* Gives indices to split the data into train and test sets\n",
    "* All observations are used as both training and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = cross_validation.KFold(10, n_folds=5)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    print \"TRAIN:\", train_index, \"TEST:\", test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(10, n_folds=5, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    print \"TRAIN:\", train_index, \"TEST:\", test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read much more about the dataset [here](http://en.wikipedia.org/wiki/Iris_flower_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demean\n",
    "X = data.data - data.data.mean(0)\n",
    "# normalize by maximum\n",
    "X /= X.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put this into 1-of-N encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = np.zeros((len(X), 3))\n",
    "target[np.arange(len(X)),data.target] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use train_test_split to split the data up into training, testing, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(feature_train, feature_test,\n",
    " target_train, target_test) = cross_validation.train_test_split(X, target, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_train.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_validate.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to split based on maintaining the same percentage of the target labels, you can use [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split the training data into training and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(feature_train, feature_validate,\n",
    " target_train, target_validate) = cross_validation.train_test_split(feature_train, \n",
    "                                                                    target_train, \n",
    "                                                                    test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = MLP(5, .1, outtype=\"softmax\")\n",
    "net.earlystopping(feature_train, target_train, feature_validate, \n",
    "                  target_validate, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.confusion_matrix(feature_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.confusion_matrix(feature_test, target_test, summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back-Propagation Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output, $y$, is a function of $x$, $g(\\cdot)$, and the weights\n",
    "* The weights will be denoted $v$ and $w$ for the first and second layers\n",
    "* $i$ is the index over the input nodes, $j$ is the index over the hidden layer neurons, $k$ is the index over the output neurons\n",
    "* First let's write the error function\n",
    "\n",
    "$$\\begin{aligned}E(\\boldsymbol{w}) & =\\frac{1}{2}\\sum_{k=1}^N\\left(t_k-y_k\\right)^2 \\cr\n",
    "& = \\frac{1}{2}\\sum_k{\\left[t_k-g\\left(\\sum_j w_{jk}a_j \\right)\\right]^2}\\end{aligned}$$\n",
    "\n",
    "where $a_j$ is the output from the hidden layer neurons\n",
    "\n",
    "* For the moment, let's ignore the hidden layer and work with the perceptron\n",
    "\n",
    "$$\\begin{aligned}E(\\boldsymbol{w}) & =\\frac{1}{2}\\sum_{k=1}^N\\left(t_k-y_k\\right)^2 \\cr\n",
    "& = \\frac{1}{2}\\sum_k{\\left[t_k-g\\left(\\sum_j w_{jk}x_j \\right)\\right]^2}\\end{aligned}$$\n",
    "\n",
    "* Since we will use gradient descent, unsurprisingly, we will need the gradient\n",
    "* Let's remind ourselves, what is the gradient again?\n",
    "* Recall that $g$ was the binary activation function and, thus, not differentiable, so we ignore it in the below\n",
    "* We adjust the weights to reduce the errors, thus we need\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\\frac{\\partial E}{\\partial w_{ik}} & = \\frac{\\partial}{\\partial w_{ik}}\\left(\\frac{1}{2}\\sum_k\\left(t_k-\\sum_j w_{jk}x_j\\right)^2\\right) \\cr\n",
    "& = \\frac{1}{2}\\sum_k2(t_k-y_k)\\frac{\\partial}{\\partial w_{ik}}\\left(t_k-\\sum_j w_{jk}x_j\\right) \\cr\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "note that\n",
    "\n",
    "$$\\frac{\\partial t_k}{\\partial w_{ik}}=0$$\n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial}{\\partial w_{ik}}\\sum_jw_{jk}x_j$\n",
    "\n",
    "is only non-zero when $i=j$, thus\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{ik}}=x_i$$\n",
    "\n",
    "so that we have\n",
    "\n",
    "$$\\begin{aligned}\\frac{\\partial E}{\\partial w_{ik}} & = \\sum_k(t_k-y_k)\\left(-x_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To make our errors smaller, we follow the gradient \"downhill\" such that (including the learning rate)\n",
    "\n",
    "$$w_{ik}\\leftarrow w_{ik}+\\eta(t_k-y_k)x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with the Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the above, it is clear that we need a differentiable $g$ to obtain $\\frac{\\partial g}{\\partial w_{ik}}$\n",
    "$$a = g(h)=\\frac{1}{1+e^{-\\beta h}}$$\n",
    "* The derivative of $g$ has a simple form\n",
    "$$\\begin{aligned}\n",
    "g^{\\prime}(h) & = \\frac{d}{dh}\\frac{1}{1+e^{-\\beta h}} \\cr\n",
    "& = \\frac{d}{dh}(1+e^{-\\beta h})^{-1} \\cr\n",
    "& = -(1+e^{-\\beta h})^{-2}(-\\beta e^{-\\beta h}) \\cr\n",
    "& = \\frac{\\beta e^{-\\beta h}}{(1+e^{-\\beta h})^{2}} \\cr\n",
    "& = \\beta\\frac{1}{1+e^{-\\beta h}}\\frac{ e^{-\\beta h}}{1+e^{-\\beta h}} \\cr\n",
    "& = \\beta\\frac{1}{1+e^{-\\beta h}}\\left(\\frac{1 + e^{-\\beta h} - 1}{1+e^{-\\beta h}}\\right) \\cr\n",
    "& = \\beta g(h)(1-g(h)) \\cr\n",
    "& = \\beta a(1-a) \\cr\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back-Propagation of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To use gradient-descent in the MLP, we need the partials of the errors with respect to each weight\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}}=\\frac{\\partial E}{\\partial h_k}\\frac{\\partial h_k}{\\partial w_{jk}}$$\n",
    "\n",
    "where $h_k=\\sum_lw_{lk}a_l$ is the input to output-layer neuron $k$\n",
    "* Ie., this is the weighted average of the activations of the hidden layer neurons, using second-layer weights\n",
    "* Taking the second part of partials, we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "{\\partial h_k}\\frac{\\partial h_k}{\\partial w_{jk}} & =\\frac{\\partial \\sum_l w_{lk}a_l}{\\partial w_{jk}} \\cr\n",
    "& = \\sum_l\\frac{\\partial w_{lk}a_l}{\\partial w_{jk}} \\cr\n",
    "& = a_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* The first term is referred to as the **error** or **delta term**\n",
    "\n",
    "$$\\delta_0 = \\frac{\\partial E}{\\partial h_k}$$\n",
    "\n",
    "* We need to unpack this, because we don't know the inputs just the outputs\n",
    "\n",
    "$$\\delta_0 = \\frac{\\partial E}{\\partial h_k}= \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k}$$\n",
    "\n",
    "where the output of the output-layer neuron $k$\n",
    "\n",
    "$$y_k = g(h_k)=g\\left(\\sum_j w_{jk}a_j\\right)$$\n",
    "\n",
    "Plugging-in the derivatives we already have for $\\delta_0$ gives\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\delta_0 & = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k} \\cr\n",
    "& = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2}\\sum_k{\\left(t_k-g\\left(\\sum_j w_{jk}x_j\\right)\\right)}\\right] \\frac{\\partial g(h_k)}{\\partial h_k} \\cr\n",
    "& = (g(h_k)-t_k)g^{\\prime}(h_k) \\cr\n",
    "& = (y_k - t_k)g^{\\prime}(h_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We already have $g^\\prime(h_k)$ so we can put it all together to give the update step for the second-layer weights\n",
    "\n",
    "$$w_{jk}\\leftarrow w_{jk} - \\eta \\frac{\\partial E}{\\partial w_{jk}}$$\n",
    "\n",
    "The missing piece is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial E}{\\partial w_{jk}} &= \\delta_0a_j \\cr\n",
    "&= (y_k - t_k)y_k(1-y_k)a_j\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we need the first layer weights - the hidden weights $v_{jk}$ \n",
    "* Remember we are going *back* propagation\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\delta_h &= \\sum_k \\frac{\\partial E}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_j} \\cr\n",
    "&= \\sum_k\\delta_0 \\frac{\\partial h_k}{\\partial h_j}\n",
    "\\end{aligned}$$\n",
    "\n",
    "* One thing to keep in mind, inputs to the output layer neurons come from the activation of the weighted average hidden layer neurons, using the second-layer weights\n",
    "\n",
    "$$h_k = g(\\sum_lw_{lk}h_l)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\frac{\\partial h_k}{\\partial h_j}=\\frac{\\partial g(\\sum_l w_{lk}h_l)}{h_j}$$\n",
    "\n",
    "Noting that $\\frac{\\partial h_i}{\\partial h_j} = 0$ if $i\\neq l$ \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial h_k}{\\partial h_j}&=w_{jk}g^{\\prime}(a_j) \\cr\n",
    "&=w_{jk}a_j(1-a_j)\n",
    "\\end{aligned}$$\n",
    "\n",
    "This gives a **delta term**\n",
    "\n",
    "$$\\delta_h = a_j(1-a_j)\\sum_k \\delta_0 w_{jk}$$\n",
    "\n",
    "So the update rule $v_{ij}\\leftarrow v_{ij} - \\eta\\frac{\\partial E}{\\partial v_{ij}}$ needs\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial v_{ij}} = a_j(1-a_j)(\\sum_k \\delta_0w_{jk})x_i$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
